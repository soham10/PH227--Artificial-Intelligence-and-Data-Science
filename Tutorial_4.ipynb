{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4\n",
    "## Name: Soham Sahasrabuddhe\n",
    "## Roll Number: 23B1848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining activation function\n",
    "def step(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Dictionary for input of activation\n",
    "activation_functions = {\n",
    "    'step': step,\n",
    "    'sigmoid': sigmoid,\n",
    "    'relu': relu,\n",
    "    'tanh': tanh\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation function chosen: step\n"
     ]
    }
   ],
   "source": [
    "# User inputs on size, weights, biases and activation\n",
    "n_i = int(input(\"Neurons in the input layer: \"))\n",
    "n_h = int(input(\"Neurons in the hidden layer: \"))\n",
    "n_o = int(input(\"Neurons in the output layer: \"))\n",
    "\n",
    "m_h = np.array([int(input(f'Number of neurons in the hidden layer {i+1}: ')) for i in range(n_h)])\n",
    "\n",
    "inputs = np.array([float(input(f'Enter real input value {i+1}: ')) for i in range(n_i)])\n",
    "\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "# input to hidden layer\n",
    "weights.append(np.array([[float(input(f\"Enter weight for Input layer to first hidden layer i.e. W(1)_{i + 1}{j + 1}: \")) for j in range(m_h[0])] for i in range(n_i)]))\n",
    "biases.append(np.array([float(input(f\"Enter bias for Input layer to first hidden layer i.e. b(1)_{i + 1}: \")) for i in range(m_h[0])]))\n",
    "\n",
    "# hidden layers\n",
    "for k in range(1,n_h):\n",
    "    weights.append(np.array([[float(input(f\"Enter weight for Hidden layer {k} to Hidden Layer {k + 1} i.e. W({k + 1})_{i + 1}{j + 1}: \")) for j in range(m_h[k])] for i in range(m_h[k - 1])]))\n",
    "    biases.append(np.array([float(input(f\"Enter bias for Hidden layer {k} to Hidden Layer {k + 1} i.e. b({k + 1})_{i +1}: \")) for i in range(m_h[k])]))\n",
    "\n",
    "# hidden to output layer\n",
    "weights.append(np.array([[float(input(f\"Enter weight for Hidden layer {n_h} to Output layer i.e. W({n_h + 1})_{i + 1}{j +1}: \")) for j in range(n_o)] for i in range(m_h[n_h- 1])]))\n",
    "biases.append(np.array([float(input(f\"Enter bias for Hidden layer {n_h} to Output layer i.e. b({n_h + 1})_{i + 1}: \")) for i in range(n_o)]))\n",
    "\n",
    "# Activation\n",
    "activation_user = input(\"Choose appropriate function out of step, relu, sigmoid, and tanh: \").lower()  # For consistency if user types uppercase characters\n",
    "activation = activation_functions.get(activation_user)\n",
    "if activation is None:\n",
    "    print(\"Invalid activation function. Defaulting to sigmoid.\")\n",
    "    activation = sigmoid\n",
    "else:\n",
    "    print(f\"Activation function chosen: {activation_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  [3.]\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagation\n",
    "input_layer = inputs\n",
    "\n",
    "for k in range(n_h + 1):\n",
    "    output_layer = np.dot(input_layer, weights[k]) + biases[k]\n",
    "    input_layer = activation(output_layer) if k < n_h else output_layer\n",
    "\n",
    "final_output = input_layer\n",
    "print(\"Output: \", final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and defining the dataset\n",
    "N = 20\n",
    "\n",
    "x_rnd = np.random.rand(N*N)\n",
    "X_1 = np.linspace(-0.5,0.5,N)\n",
    "X_2 = np.linspace(-0.5,0.5,N)\n",
    "x1,x2 = np.meshgrid(X_1,X_2)\n",
    "x1 = x1.flatten()\n",
    "x2 = x2.flatten()\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "y = x1**2 + 2*x1 - x2 + x2**2 + 0.5*x_rnd\n",
    "\n",
    "# Division into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=300, test_size=100, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network(X_train, y_train, X_test, y_test, layers, neurons, atv, epochs, batchsize):\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(neurons, activation = atv)) # Input\n",
    "\n",
    "    for i in range(layers):   #Hidden\n",
    "        model.add(Dense(neurons, activation = atv))\n",
    "\n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    model.fit(X_train, y_train, epochs = epochs, batch_size = batchsize, verbose=0) # Training\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test, verbose = 0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Hidden Layers  Neurons  Activation  Epochs  Batch Size  Test Loss\n",
      "0                1       10        relu      10          32   0.148151\n",
      "1                1       10        relu      10          64   0.390454\n",
      "2                1       10        relu      10         128   0.561285\n",
      "3                1       10        relu      20          32    0.03409\n",
      "4                1       10        relu      20          64   0.180774\n",
      "..             ...      ...         ...     ...         ...        ...\n",
      "320              3       30     sigmoid      50         128   0.365373\n",
      "321              3       30     sigmoid     100          32   0.036598\n",
      "322              3       30     sigmoid     100          64    0.03326\n",
      "323              3       30     sigmoid     100         128   0.041554\n",
      "324  Hidden Layers  Neurons  Activation  Epochs  Batch Size  Test Loss\n",
      "\n",
      "[325 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "hidden_confg = [1, 2, 3]\n",
    "neuron_confg = [10, 20 ,30]\n",
    "activation =  ['relu', 'tanh', 'sigmoid']\n",
    "epochs = [10, 20, 50, 100]\n",
    "batch_size = [32, 64, 128]\n",
    "results = []\n",
    "\n",
    "for layer in hidden_confg:\n",
    "    for neuron in neuron_confg:\n",
    "        for act in activation:\n",
    "            for ep in epochs:\n",
    "                for bs in batch_size:\n",
    "                    test_loss = training_network(X_train, y_train, X_test, y_test, layers = layer, neurons=neuron, atv=act, epochs=ep, batchsize=bs)\n",
    "                    results.append([layer, neuron, act,ep,bs, test_loss])\n",
    "\n",
    "results.append({\n",
    "    'Hidden Layers': layer,\n",
    "    'Neurons': neuron,\n",
    "    'Activation': act,\n",
    "    'Epochs': ep,\n",
    "    'Batch Size': bs,\n",
    "    'Test Loss': test_loss\n",
    "})\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Hidden Layers', 'Neurons', 'Activation', 'Epochs','Batch Size', 'Test Loss']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 60, 'name': 'Liver Disorders', 'repository_url': 'https://archive.ics.uci.edu/dataset/60/liver+disorders', 'data_url': 'https://archive.ics.uci.edu/static/public/60/data.csv', 'abstract': 'BUPA Medical Research Ltd. database donated by Richard S. Forsyth', 'area': 'Health and Medicine', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 345, 'num_features': 5, 'feature_types': ['Categorical', 'Integer', 'Real'], 'demographics': [], 'target_col': ['drinks'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2016, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C54G67', 'creators': [], 'intro_paper': None, 'additional_info': {'summary': 'The first 5 variables are all blood tests which are thought to be sensitive to liver disorders that might arise from excessive alcohol consumption. Each line in the dataset constitutes the record of a single male individual.\\n\\nImportant note: The 7th field (selector) has been widely misinterpreted in the past as a dependent variable representing presence or absence of a liver disorder. This is incorrect [1]. The 7th field was created by BUPA researchers as a train/test selector. It is not suitable as a dependent variable for classification. The dataset does not contain any variable representing presence or absence of a liver disorder. Researchers who wish to use this dataset as a classification benchmark should follow the method used in experiments by the donor (Forsyth & Rada, 1986, Machine learning: applications in expert systems and information retrieval) and others (e.g. Turney, 1995, Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm), who used the 6th field (drinks), after dichotomising, as a dependent variable for classification. Because of widespread misinterpretation in the past, researchers should take care to state their method clearly.\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1. mcv mean corpuscular volume\\n2. alkphos alkaline phosphotase\\n3. sgpt alanine aminotransferase\\n4. sgot aspartate aminotransferase\\n5. gammagt gamma-glutamyl transpeptidase\\n6. drinks number of half-pint equivalents of alcoholic beverages drunk per day\\n7. selector field created by the BUPA researchers to split the data into train/test sets', 'citation': None}}\n",
      "       name     role         type demographic  \\\n",
      "0       mcv  Feature   Continuous        None   \n",
      "1   alkphos  Feature   Continuous        None   \n",
      "2      sgpt  Feature   Continuous        None   \n",
      "3      sgot  Feature   Continuous        None   \n",
      "4   gammagt  Feature   Continuous        None   \n",
      "5    drinks   Target   Continuous        None   \n",
      "6  selector    Other  Categorical        None   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                            mean corpuscular volume  None             no  \n",
      "1                               alkaline phosphotase  None             no  \n",
      "2                           alanine aminotransferase  None             no  \n",
      "3                         aspartate aminotransferase  None             no  \n",
      "4                      gamma-glutamyl transpeptidase  None             no  \n",
      "5  number of half-pint equivalents of alcoholic b...  None             no  \n",
      "6  field created by the BUPA researchers to split...  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "liver_disorders = fetch_ucirepo(id=60) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = np.array(liver_disorders.data.features)\n",
    "y = np.array(liver_disorders.data.targets)\n",
    "  \n",
    "# metadata \n",
    "print(liver_disorders.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(liver_disorders.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=273, test_size=72, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f1991598310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f1991598280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "                0        1           2          3\n",
      "0               1       10        relu  25.832396\n",
      "1               1       10        tanh   9.816648\n",
      "2               1       10     sigmoid  10.086171\n",
      "3               1       20        relu  11.204685\n",
      "4               1       20        tanh   8.832933\n",
      "5               1       20     sigmoid   9.691667\n",
      "6               1       30        relu  11.102077\n",
      "7               1       30        tanh   8.739369\n",
      "8               1       30     sigmoid   9.024007\n",
      "9               2       10        relu  10.659854\n",
      "10              2       10        tanh  10.452765\n",
      "11              2       10     sigmoid  10.662361\n",
      "12              2       20        relu  13.061833\n",
      "13              2       20        tanh   9.606407\n",
      "14              2       20     sigmoid   10.06094\n",
      "15              2       30        relu  10.346254\n",
      "16              2       30        tanh     9.6021\n",
      "17              2       30     sigmoid   9.019244\n",
      "18              3       10        relu    9.99665\n",
      "19              3       10        tanh   8.861134\n",
      "20              3       10     sigmoid  10.559407\n",
      "21              3       20        relu   9.717434\n",
      "22              3       20        tanh  10.533259\n",
      "23              3       20     sigmoid  10.508048\n",
      "24              3       30        relu  11.460978\n",
      "25              3       30        tanh  10.465952\n",
      "26              3       30     sigmoid   9.522939\n",
      "27  Hidden Layers  Neurons  Activation  Test Loss\n"
     ]
    }
   ],
   "source": [
    "hidden_confg = [1, 2, 3]\n",
    "neuron_confg = [10, 20 ,30]\n",
    "activation =  ['relu', 'tanh', 'sigmoid']\n",
    "results = []\n",
    "\n",
    "for layer in hidden_confg:\n",
    "    for neuron in neuron_confg:\n",
    "        for act in activation:\n",
    "            test_loss = training_network(X_train, y_train, X_test, y_test, layers = layer, neurons=neuron, atv=act, epochs=100, batchsize=64)\n",
    "            results.append([layer, neuron, act, test_loss])\n",
    "\n",
    "results.append({\n",
    "    'Hidden Layers': layer,\n",
    "    'Neurons': neuron,\n",
    "    'Activation': act,\n",
    "    'Test Loss': test_loss\n",
    "})\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
